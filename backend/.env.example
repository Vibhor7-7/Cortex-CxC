# CORTEX Backend Configuration
#
# SETUP INSTRUCTIONS FOR NEW CONTRIBUTORS:
# 1. Copy this file to .env: cp .env.example .env
# 2. Install Ollama: https://ollama.com
# 3. Pull required models: ollama pull qwen2.5 && ollama pull nomic-embed-text
# 4. Start Ollama: ollama serve
# 5. No API keys required â€” fully local!

# Database Configuration
DATABASE_URL=sqlite:///./cortex.db

# Ollama Configuration (local LLM for summarization + embeddings)
# Requires: ollama serve running
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# UMAP Configuration
UMAP_N_NEIGHBORS=15
UMAP_MIN_DIST=0.1

# Clustering Configuration
N_CLUSTERS=5

# Server Configuration
HOST=0.0.0.0
PORT=8000
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# Environment
ENVIRONMENT=development

# ============================================================================
# Backboard.io Integration (Optional - for retrieval quality evaluation)
# ============================================================================
# Get your API key from: https://app.backboard.io/settings/api-keys

# Backboard API key (required for evaluation features)
# BACKBOARD_API_KEY=your_api_key_here

# Backboard API base URL (default: https://app.backboard.io/api)
BACKBOARD_API_URL=https://app.backboard.io/api

# Enable/disable Backboard features (default: false, auto-enabled when API key is set)
BACKBOARD_ENABLED=false

# Model to use for evaluation (default: gpt-4.1)
BACKBOARD_EVAL_MODEL=gpt-4.1

# MCP Guard Configuration (filters low-confidence memories before context injection)
# Enable/disable guard (default: true when API key is present)
BACKBOARD_GUARD_ENABLED=true

# Minimum confidence threshold for memories (0.0-1.0, default: 0.5)
BACKBOARD_GUARD_THRESHOLD=0.5

# Log blocked memories for debugging (default: true)
BACKBOARD_GUARD_LOG_BLOCKED=true